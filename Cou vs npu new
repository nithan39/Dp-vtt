import time
import torch
import whisper
import psutil
import os

# Load the model
model = whisper.load_model("tiny")  # or base, small, medium, large

# Prepare test audio
audio_path = "sample.wav"
audio = whisper.load_audio(audio_path)
audio = whisper.pad_or_trim(audio)
mel = whisper.log_mel_spectrogram(audio).to(model.device)

# Measure inference time
start_time = time.time()
result = model.decode(mel)
end_time = time.time()

# Compute latency and throughput
latency = end_time - start_time
throughput = 1 / latency

# Memory usage
process = psutil.Process(os.getpid())
memory_mb = process.memory_info().rss / (1024 ** 2)

print(f"Latency: {latency:.3f} s")
print(f"Throughput: {throughput:.2f} inferences/sec")
print(f"Memory Usage: {memory_mb:.2f} MB")

….....…………….

import time
from hailo_platform import Device

hef_path = "tinyencoder.hef"

with Device() as dev:
    hef = dev.create_hef(hef_path)
    net_group = hef.configure()
    net = net_group.get_networks()[0]

    input_tensor = net.allocate_input_tensors()[0]
    output_tensor = net.allocate_output_tensors()[0]

    # Warmup
    for _ in range(10):
        net.infer(input_tensor, output_tensor)

    start = time.time()
    for _ in range(100):
        net.infer(input_tensor, output_tensor)
    end = time.time()

    avg_latency = (end - start) / 100
    print(f"Average latency: {avg_latency*1000:.2f} ms")
    print(f"Throughput: {1/avg_latency:.2f} inferences/sec")

....,,,..............,.....,.,........

import matplotlib.pyplot as plt

devices = ["CPU", "Hailo NPU"]
latencies = [0.48, 0.012]  # example values
throughputs = [2.08, 83.3]

fig, ax1 = plt.subplots()
ax1.bar(devices, latencies)
ax1.set_ylabel('Latency (s)')
plt.show()
