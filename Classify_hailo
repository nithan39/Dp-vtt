def classify(self, image_path):
    """Run classification on an image"""

    # Get input info from the configured network group
    input_infos = self.network_group.get_input_vstream_infos()
    input_info = input_infos[0]
    input_shape = input_info.shape                  # e.g. (224,224,3) or (1,224,224,3)
    input_name = input_info.name
    print(f"\nModel expects input shape: {input_shape}, input name: {input_name}")

    # Determine target size (height, width)
    if len(input_shape) == 4:   # NHWC (batch, H, W, C)
        target_size = (input_shape[1], input_shape[2])
    elif len(input_shape) == 3: # HWC
        target_size = (input_shape[0], input_shape[1])
    else:
        target_size = (224, 224)

    # Preprocess image (returns uint8 HWC)
    input_data = self.preprocess_image(image_path, target_size)
    input_data = input_data.astype(np.uint8)

    # Add batch dimension if model expects one
    if len(input_shape) == 4 and len(input_data.shape) == 3:
        input_data = np.expand_dims(input_data, axis=0)

    print(f"Final input shape before batching: {input_data.shape}, dtype: {input_data.dtype}")

    # determine expected frame count (some HEFs expect many frames packed)
    expected_frame_count = None
    # try to read a size/frames field if available
    frame_count_attr = None
    if hasattr(input_info, "size") and input_info.size is not None:
        # input_info.size is total bytes for vstream; compute frames if possible
        single_frame_elems = int(np.prod(input_shape[-3:]))  # H*W*C
        try:
            expected_frame_count = int(input_info.size // single_frame_elems)
            frame_count_attr = "size"
        except Exception:
            expected_frame_count = None

    # Fallback: try a common attribute if present
    if expected_frame_count is None:
        if hasattr(input_info, "frame_count"):
            expected_frame_count = int(getattr(input_info, "frame_count"))
            frame_count_attr = "frame_count"

    # final fallback -> assume batch 1
    if expected_frame_count is None:
        expected_frame_count = 1

    print(f"[DEBUG] expected_frame_count (from {frame_count_attr or 'fallback'}): {expected_frame_count}")

    # Repeat the single image to match expected batch/frame count if needed
    if input_data.shape[0] != expected_frame_count:
        if input_data.shape[0] == 1:
            input_data = np.repeat(input_data, expected_frame_count, axis=0)
            print(f"Repeated input to match expected frames. New shape: {input_data.shape}")
        else:
            # If user supplied more than 1 frame but not expected_frame_count, try to tile/truncate
            if input_data.shape[0] < expected_frame_count:
                input_data = np.repeat(input_data, int(np.ceil(expected_frame_count / input_data.shape[0])), axis=0)
                input_data = input_data[:expected_frame_count]
            else:
                input_data = input_data[:expected_frame_count]

    # Prepare input dictionary (handle dict/list param types)
    if isinstance(self.input_vstreams_params, dict):
        input_name = list(self.input_vstreams_params.keys())[0]
    else:
        param = self.input_vstreams_params[0]
        input_name = getattr(param, "name", param)

    input_dict = {input_name: input_data}
    print(f"Final input shape after batching: {input_data.shape} -> building input_dict with key: {input_name}")

    # Activate network group with context manager, then create/use InferVStreams
    print("Activating network group and running inference...")
    import time
    start_time = time.time()

    # Use activate() as a context manager — ensures activation lives for the duration
    # (some HailoRT versions require being in the same context when using vstreams)
    with self.network_group.activate():
        with InferVStreams(self.network_group,
                          self.input_vstreams_params,
                          self.output_vstreams_params) as infer_pipeline:
            output_dict = infer_pipeline.infer(input_dict)

    inference_time = (time.time() - start_time) * 1000
    print(f"✓ Inference completed in {inference_time:.2f} ms")

    # Process output
    output_name = list(output_dict.keys())[0]
    output_data = output_dict[output_name]

    print(f"\nOutput shape: {output_data.shape}")
    print(f"Output dtype: {output_data.dtype}")

    # Flatten and softmax
    predictions = output_data.flatten()
    predictions = self.softmax(predictions)

    return predictions
